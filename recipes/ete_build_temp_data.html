
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Details-on-workflow-execution-and-intermediate-data">Details on workflow execution and intermediate data<a class="anchor-link" href="#Details-on-workflow-execution-and-intermediate-data">&#182;</a></h1><p>Here we describe how intermediate steps are handled <code>ete-build</code> and how to access the intermediate files and data generated by the different tasks and jobs.</p>
<h2 id="Requirements">Requirements<a class="anchor-link" href="#Requirements">&#182;</a></h2><ul>
<li>ete3 </li>
<li>ete_external_apps</li>
</ul>
<h2 id="Concepts">Concepts<a class="anchor-link" href="#Concepts">&#182;</a></h2><p>ete-build is a wrapper tool that allows to pipeline the execution of multiple programs. Although a final tree and alignment is reported as the main result o workflows, many other operations took place in the background.</p>
<p>In order to understand how ete-build works, consider the following facts:</p>
<ul>
<li><p>A workflow is composed of many <em>Tasks</em></p>
</li>
<li><p>Each <em>Task</em> is composed of none or multiple <em>Jobs</em>, plus some post-analysis operations (i.e. parsing, cleaning, etc.)</p>
</li>
<li><p>Each Job represents a call to an external program</p>
</li>
<li><p>When running a workflow using the <code>mafft_linsi</code> aligner, this is translated into a Mafft task that will call the <code>mafft</code> binary with the arguments and options under the <code>mafft_linsi</code> configuration block.</p>
</li>
<li><p>Each Job in task has a unique hash-id built using input data, program type and program arguments as source. A minimal change in one of the options would generate a different job-id.</p>
</li>
<li><p>Similarly, each task is assigned with a unique hash-id based on the configuration of the task and the ids of its sibling jobs.</p>
</li>
<li><p>all tasks and jobs ids, as well as the resulting data, are stored in a SQLITE database. A unique ID is also assigned to each piece of data generated (i.e. Multiple Sequence Alignment, Tree or trimmed alignment).</p>
</li>
</ul>
<p>Altogether, this system is what permits reusing previous results when resuming an analysis. If a new tasks or job is registered that is present the database, the stored output will be used.</p>
<h2 id="Common-questions">Common questions<a class="anchor-link" href="#Common-questions">&#182;</a></h2><h3 id="1.-Where-to-find-job-and-task-IDs">1. Where to find job and task IDs<a class="anchor-link" href="#1.-Where-to-find-job-and-task-IDs">&#182;</a></h3><p>There are several ways:</p>
<ul>
<li>while monitoring the execution, a 8-characters id is shown for each task and job. </li>
<li><p>After execution, a file called <code>commands.log</code> will be present in the output directory. It has the following tab-delimited format:</p>
<p><code>| TaskType | TaskId | JobName | JobID | command line used (if relevant)|</code></p>
</li>
</ul>
<h3 id="2.-Where-to-find-intermediate-data">2. Where to find intermediate data<a class="anchor-link" href="#2.-Where-to-find-intermediate-data">&#182;</a></h3><p>All intermediate operations occur in the <code>tasks/</code> directory. Within <code>tasks/</code>, each Job and some Tasks store and process intermediate data. The name of subdirectories corresponds to Job or Task IDs.</p>
<p>The <code>input/</code> folder is used to dump previously generated data with is stored in the database and that is required as input for other tasks. If a Job requires data files generated in previous tasks, those files will be referred using their corresponding data IDs and will be dumped in the <code>input/</code> directory when necessary.</p>
<h3 id="3.-How-to-inspect-data">3. How to inspect data<a class="anchor-link" href="#3.-How-to-inspect-data">&#182;</a></h3><p>All job directories follow the same basic structure. They contain:</p>
<ul>
<li>a file <code>__cmd__</code> with the command line used to launch the job</li>
<li><code>__stdout__</code> and <code>__stderr__</code> files capturing job output</li>
<li>a <code>__time__</code> file recording start and finish time of the job</li>
<li>a <code>__status__</code> file reporting a single letter status about if the job is (D)one, (R)unning or has (E)rrors.</li>
<li>Any other additional files produced during job execution</li>
</ul>

</div>
</div>
</div>